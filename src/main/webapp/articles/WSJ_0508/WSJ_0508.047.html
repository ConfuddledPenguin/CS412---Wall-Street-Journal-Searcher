<div class="article">
	<h3>Technology: MIT Scientists Rethink Artificial Intelligence</h3>
	<div class="article-info">
		<ul>
			<li>Author: David Stipp</li>
			<li>Date: 05/08/90</li>
		</ul>
	</div>
	<p class="article-leader">CAMBRIDGE, Mass. -- As futuristic robots go, Genghis isn't
much to look at.
   The six-legged, foot-long contraption could pass as a
sixth-grader's wire sculpture of an ant. The researchers who
built it whimsically designed the thing to chase people --
like its namesake, Mr. Khan. As an attack robot, though,
Genghis isn't quite up to speed. It lurches slowly across a
floor at Massachusetts Institute of Technology and almost
crashes into a wall. Still, its designers are very proud of
what their creation can do -- for unlike most robots, Genghis
doesn't have a brain.</p>
	<div class="article-body"><p>At least not in the ordinary computing sense of a
centralized control program. Instead, Genghis has a network
of small, simple control programs, each devoted to a single
function such as lifting a leg. They work independently but
are connected to interact something like bees in a hive. From
their collective action, walking "emerges" -- as a store of
honey emerges from bees' interactions.</p>
<p>Genghis may lurch, but researchers at MIT's artificial
intelligence laboratory are using it to mount a frontal
assault on the conventional wisdom in artificial
intelligence, the attempt to mimic human thinking with
computers. Since the 1950s, AI research has been dominated by
big, complex systems aimed at copying higher thought
processes, such as the ability to understand this sentence.
Despite some successes, the approach so far has emulated
human intelligence about as well as early aircraft flew by
flapping bird-like wings.</p>
<p>Now at MIT and some other places, AI researchers are
trying a radically different approach. Their idea, called
"bottom-up" AI, is to build interacting networks of many
relatively simple devices, or "agents." The agents might be a
network of little software functionaries running on
off-the-shelf chips, as in Genghis. Or they could be
separate, interacting machines -- the MIT researchers
envision "robot insect societies" that clean up oil spills,
build dams, explore other planets or pounce on dust balls
under the radiator.</p>
<p>Such swarms of cheap little robots "would be sort of like
parallel processors," fast, new computers that perform many
calculations at once, says Anita Flynn, one of the MIT team.
"Only they would be doing mechanical work."</p>
<p>If set up right -- a big if, critics say -- these networks
of agents can give rise to surprisingly complex and flexible
collective action. When hit by a transient power outage,
Genghis collapses and flails its legs helplessly; the ability
to coordinate them has been erased from the agents inside the
robot. Soon, however, the flailings begin to look more
coordinated. Within a couple of minutes, the agents gradually
"learn" again how to make Genghis stand up and go.</p>
<p>The process uncannily resembles an accelerated version of
a six-legged infant learning to walk. What happens is that
"the agents learn how to avoid receiving negative feedback,"
which they get from sensors on Genghis's belly when it falls
down, says Pattie Maes, an MIT visitor from Belgium who
recently added the learning ability to Genghis's repertoire.</p>
<p>In a sense, the agents program themselves to walk, trying
different actions and learning from the sensors -- and from
communication among themselves -- which movements work and
which don't. For example, it learns to always keep at least
three legs on the ground, adopting a pattern of movement seen
in insects and familiar to entomologists called the
alternating tripod gait.</p>
<p>A conventional robot could handle such challenges as
learning to walk only if they were foreseen by its designers
and precisely accounted for in its master control program.
But foreseeing all the problems that crop up in real-world
situations is almost impossible. Thus, robots, and
conventional AI systems in general, work best in tightly
constrained environments where surprises are minimized.
Examples include assembly-line robots and chess-playing
programs. Similarly, AI programs once touted as potential
rivals of human experts haven't lived up to such expectations
because they're often fooled by real-world complexities.</p>
<p>The bottom-up approach won't necessarily overcome such
problems. But it shows promise at cracking them, partly
because it "decentralizes things so you don't have a big
program that breaks" when something unexpected happens, says
MIT's Marvin Minsky, considered one of AI's leading lights.
Mr. Minsky isn't directly involved with MIT's new robots, but
his theory that the mind is a society of agents has helped
inspire them.</p>
<p>MIT's robot researchers don't plan to tackle human
intelligence head-on. They would be happy to replicate insect
smarts. Thus, they've created Squirt, a matchbox-sized robot
with wheels that acts something like a cockroach: It hides in
dark corners, ventures out in the direction of noises (after
the noises are long gone) and then tries to hide again near
the noises' origin.</p>
<p>Now they're planning gnat-sized robots for everything from
attacking potato bugs in the garden to clearing clogged
arteries. In offices decorated with toy robots, they talk
excitedly of doing for AI and robots what personal computers
did for data processing. In keeping with that goal, the
group's hardware guru, Colin Angle, and a friend built a
joystick-controlled, insect-like robot that can walk, pick up
small objects, do handstands and flip. Built in two days for
less than $200, it has balsa-wood legs and is held together
with duct tape.</p>
<p>Emulating insects is a "noble goal for AI practitioners,"
says Rodney A. Brooks, a transplanted Australian who's
leading the charge of the bottom-up robots at MIT. He notes
that it took evolution about three billion years to go from
single cells to insects, and only about one-sixth of that
time to go from insects to humans. That suggests, he says,
that humans' higher reasoning powers won't be too hard for AI
researchers to replicate once they can mimic insects' ability
to cope with the world's blooming, buzzing confusion.</p>
<p>But that doesn't quite compute, some AI experts say. "Most
of what enables us to get by in the world, doing the things
that other animals can't do, is knowledge we've developed
culturally," in contrast to insects' lower brain powers, says
AI researcher Douglas Lenat. The bottom-up approach "is
important," he adds. "But we won't achieve true AI until we
also construct systems like Cyc," a huge AI program he's
designing that will incorporate tens of millions of
common-sense rules and other information. It's being
developed at Microelectronics & Computer Technology Corp., a
research group in Austin, Texas.</p>
<p>In any case, bottom-up ideas, which have been simmering
for decades in AI research, are now bubbling over and making
a splash. One reason is neural networks, a variation on the
bottom-up theme in which the agents are rough likenesses of
the brain's neurons. Such computers are showing promise in
computer vision, speech recognition and many other uses.</p>
<p>Danny Hillis, founder of Thinking Machines Inc., a
Cambridge, Mass., computer company, is pursuing another
bottom-up idea: artificial evolution. The method
automatically creates programs for certain tasks by randomly
generating and combining hundreds of possible programs in one
of Thinking Machines' computers. Each is tested for its
ability to perform the task; only the fittest are allowed to
survive and be recombined to form a new generation of
programs. Eventually, programs emerge from this computational
swamp that are comparable to those a human might write, says
Mr. Hillis.</p>
<p></p></div>
</div>
